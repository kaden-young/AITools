{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the 100k feather file\n",
    "df = pd.read_feather('100k_kindle_reviews.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m lemmatized_text\n\u001b[0;32m     13\u001b[0m \u001b[39m# Apply the preprocessing function to the 'reviewText' column\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mprocessed_review\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mreviewText\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(preprocess_text)\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[1;32m----> 9\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     10\u001b[0m     lemmatized_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop])\n\u001b[0;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m lemmatized_text\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\language.py:1026\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1025\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1026\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1028\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:143\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    141\u001b[0m error_handler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatch(doc)\n\u001b[0;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_annotations(doc, matches)\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:152\u001b[0m, in \u001b[0;36mAttributeRuler.match\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    150\u001b[0m matches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatcher(doc, allow_missing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, as_spans\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[39m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m matches \u001b[39m=\u001b[39m [\n\u001b[0;32m    153\u001b[0m     (\u001b[39mint\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstrings[m_id]), m_id, s, e) \u001b[39mfor\u001b[39;49;00m m_id, s, e \u001b[39min\u001b[39;49;00m matches  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    154\u001b[0m ]\n\u001b[0;32m    155\u001b[0m matches\u001b[39m.\u001b[39msort()\n\u001b[0;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m matches\n",
      "File \u001b[1;32mc:\\Users\\youngkj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:152\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    150\u001b[0m matches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatcher(doc, allow_missing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, as_spans\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    151\u001b[0m \u001b[39m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m matches \u001b[39m=\u001b[39m [\n\u001b[0;32m    153\u001b[0m     (\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[39mfor\u001b[39;00m m_id, s, e \u001b[39min\u001b[39;00m matches  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    154\u001b[0m ]\n\u001b[0;32m    155\u001b[0m matches\u001b[39m.\u001b[39msort()\n\u001b[0;32m    156\u001b[0m \u001b[39mreturn\u001b[39;00m matches\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the Spacy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to perform lemmatization and stop word removal\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the preprocessing function to the 'reviewText' column\n",
    "df['processed_review'] = df['reviewText'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit CountVectorizer to learn the vocabulary (word dictionary)\n",
    "vectorizer.fit(df['processed_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components_range = range(1, 200, 10)  # Define the range of components to test\n",
    "# explained_variance = []\n",
    "\n",
    "# for n_components in n_components_range:\n",
    "#     svd = TruncatedSVD(n_components=n_components)\n",
    "#     svd.fit(vectorizer.transform(df['processed_review']))\n",
    "#     explained_variance.append(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# # Plot the explained variance ratio to identify the elbow point\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(n_components_range, explained_variance, marker='o', linestyle='-')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Explained Variance Ratio')\n",
    "# plt.title('Elbow Method for Optimal Number of Components')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f26278aaebced278c5097c56985d85bde54a4cf6cf6ca5c930d7239c5fca84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
